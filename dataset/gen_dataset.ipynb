{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whitespace_correction import WhitespaceCorrector\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee68c0c51dca44d8b92804dba7e12ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model_semantic_similarity = AutoModel.from_pretrained('/home/css/models/NV-Embed-v2', trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "def calculate_semantic_similarity(sentence1, sentence2, max_length=32768):\n",
    "    # 对输入的两个句子进行编码\n",
    "    embeddings = model_semantic_similarity.encode([sentence1, sentence2], instruction=\"\", max_length=max_length)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity = cos_sim(embeddings[0], embeddings[1])\n",
    "    \n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/.cache/huggingface/modules/transformers_modules/NV-Embed-v2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n",
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9388136267662048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1=\"The lecturer went against the authour 's insistence for the following reasons .\"\n",
    "text_2=\"The lecturer went against the authour's insistence for the following reasons.\"\n",
    "calculate_semantic_similarity(text_1,text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(name='eo_large_byte', description='Byte-level model combining fast inference and good quality', tags=['default', 'lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_large_char', description='Character-level model combining fast inference and good quality', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_large_char_v1', description='Character-level model combining fast inference and good quality, trained with a different loss than eo_large_char', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_larger_byte', description='Larger and slower than eo_large_byte, but also more accuracte', tags=['lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_medium_byte', description='Smaller and faster than eo_large_byte, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_medium_char', description='Smaller and faster than eo_large_char, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_medium_char_v1', description='Smaller and faster than eo_large_char_v1, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='ed_large_char', description='Similar to eo_large_byte in size and quality, but slower due to its autoregressive decoder', tags=['lang::en', 'arch::encoder-decoder', 'input::char', 'output::char']),\n",
       " ModelInfo(name='ed_medium_char', description='Smaller and faster than ed_large, but less accurate', tags=['lang::en', 'arch::encoder-decoder', 'input::char', 'output::char'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceCorrector.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 21:46:45,406 [WHITESPACE CORRECTION DOWNLOAD] [INFO] eo_larger_byte is already downloaded to download directory /home/css/models/wsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/io.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_path, map_location=device)\n",
      "2024-10-28 21:46:46,624 [WHITESPACE CORRECTION] [INFO] running eo_huge_byte whitespace corrector on device NVIDIA GeForce RTX 4090 (24,217MiB memory, 8.9 compute capability, 128 multiprocessors)\n"
     ]
    }
   ],
   "source": [
    "cor = WhitespaceCorrector.from_pretrained(model=\"eo_larger_byte\", device=\"cuda:0\", download_dir=\"/home/css/models/wsc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取txt文件，wsc，保存txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5600d6da7f8243e2a1e043426f53803c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/modules/grouping.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322929f03b9446e99268194a6efe2fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################### 方法一 ，分别对数据集两个部分进行回滚\n",
    "# 文本文件路径\n",
    "tt = 1\n",
    "file_name = [\"sources.txt\", \"corrections.txt\"]\n",
    "input_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg/jfleg/\" + file_name[tt]\n",
    "output_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg/jfleg_corrected/\" + file_name[tt]\n",
    "output_file_path_rollback = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg/jfleg_rollback/\" + file_name[tt]\n",
    "\n",
    "# 打开文件并读取内容到列表\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    jfleg_lst = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "jfleg_lst_output = []\n",
    "for item in tqdm(jfleg_lst):\n",
    "    corrected_string = cor.correct_text(item)\n",
    "    jfleg_lst_output.append(corrected_string)\n",
    "\n",
    "\n",
    "jfleg_lst_output_rollback = []\n",
    "nn = 0\n",
    "for item in tqdm(jfleg_lst):\n",
    "    ori_text = item\n",
    "    cor_text = cor.correct_text(item)\n",
    "    if calculate_semantic_similarity(ori_text, cor_text) < 0.94:\n",
    "        jfleg_lst_output_rollback.append(ori_text)\n",
    "        nn += 1\n",
    "    else:\n",
    "        jfleg_lst_output_rollback.append(cor_text)\n",
    "\n",
    "# 将列表内容写入到txt文件中，最后一行不加换行符\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    for i, line in enumerate(jfleg_lst_output):\n",
    "        if i < len(jfleg_lst_output) - 1:\n",
    "            file.write(line + \"\\n\")\n",
    "        else:\n",
    "            file.write(line)\n",
    "\n",
    "with open(output_file_path_rollback, \"w\") as file:\n",
    "    for i, line in enumerate(jfleg_lst_output_rollback):\n",
    "        if i < len(jfleg_lst_output_rollback) - 1:\n",
    "            file.write(line + \"\\n\")\n",
    "        else:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
