{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whitespace_correction import WhitespaceCorrector\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from sentence_transformers.util import cos_sim\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_semantic_similarity = AutoModel.from_pretrained('/home/css/models/NV-Embed-v2', trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "# def calculate_semantic_similarity(sentence1, sentence2, max_length=32768):\n",
    "\n",
    "#     task_name_to_instruct = {\"STS\": \"Retrieve semantically similar text.\"} # STS: 语义相似度任务\n",
    "#     query_prefix = \"Instruct: \"+task_name_to_instruct[\"STS\"]+\"\\nQuery: \"\n",
    "#     # 对输入的两个句子进行编码\n",
    "#     claims_embeds = model_semantic_similarity.encode([\n",
    "#         sentence1\n",
    "#     ]\n",
    "#     , instruction=query_prefix, max_length=32768)\n",
    "\n",
    "#     contexts_embeds = model_semantic_similarity.encode([\n",
    "#             sentence2\n",
    "#         ]\n",
    "#     , instruction=query_prefix, max_length=32768)\n",
    "    \n",
    "#     # 计算余弦相似度\n",
    "#     similarity = cos_sim(claims_embeds, contexts_embeds)\n",
    "    \n",
    "#     return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf79850ee0854353b752ad9a0cc9b681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model_semantic_similarity = AutoModel.from_pretrained('/home/css/models/NV-Embed-v2', \n",
    "                                                      trust_remote_code=True, \n",
    "                                                      device_map=\"auto\",\n",
    "                                                      torch_dtype='bfloat16',\n",
    "                                                      )\n",
    "\n",
    "def calculate_semantic_similarity(sentence1, sentence2, max_length=32768):\n",
    "    # 对输入的两个句子进行编码\n",
    "    embeddings = model_semantic_similarity.encode([sentence1, sentence2], \n",
    "                                                  instruction=\"\", \n",
    "                                                  max_length=max_length)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity = cos_sim(embeddings[0], embeddings[1])\n",
    "    \n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/.cache/huggingface/modules/transformers_modules/NV-Embed-v2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n",
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9726235866546631"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_1=\"The lecturer went against the authour 's insistence for the following reasons .\"\n",
    "# text_2=\"The lecturer went against the authour's insistence for the following reasons.\"\n",
    "# calculate_semantic_similarity(text_1,text_2)\n",
    "\n",
    "text_1=\"the fact of the matter is that a lot of acadmic subjects can not be used separately .\"\n",
    "text_2=\"the fact of the matter is that a lot of acadmic subjects cannot be used separately.\"\n",
    "calculate_semantic_similarity(text_1,text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(name='eo_large_byte', description='Byte-level model combining fast inference and good quality', tags=['default', 'lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_large_char', description='Character-level model combining fast inference and good quality', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_large_char_v1', description='Character-level model combining fast inference and good quality, trained with a different loss than eo_large_char', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_larger_byte', description='Larger and slower than eo_large_byte, but also more accuracte', tags=['lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_medium_byte', description='Smaller and faster than eo_large_byte, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::byte']),\n",
       " ModelInfo(name='eo_medium_char', description='Smaller and faster than eo_large_char, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='eo_medium_char_v1', description='Smaller and faster than eo_large_char_v1, but less accurate', tags=['lang::en', 'arch::encoder-only', 'input::char']),\n",
       " ModelInfo(name='ed_large_char', description='Similar to eo_large_byte in size and quality, but slower due to its autoregressive decoder', tags=['lang::en', 'arch::encoder-decoder', 'input::char', 'output::char']),\n",
       " ModelInfo(name='ed_medium_char', description='Smaller and faster than ed_large, but less accurate', tags=['lang::en', 'arch::encoder-decoder', 'input::char', 'output::char'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceCorrector.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 17:31:16,776 [WHITESPACE CORRECTION DOWNLOAD] [INFO] eo_larger_byte is already downloaded to download directory /home/css/models/wsc\n",
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/io.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_path, map_location=device)\n",
      "2024-11-15 17:31:18,568 [WHITESPACE CORRECTION] [INFO] running eo_huge_byte whitespace corrector on device NVIDIA GeForce RTX 4090 (24,217MiB memory, 8.9 compute capability, 128 multiprocessors)\n"
     ]
    }
   ],
   "source": [
    "cor = WhitespaceCorrector.from_pretrained(model=\"eo_larger_byte\", device=\"cuda:0\", download_dir=\"/home/css/models/wsc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1读取txt文件，wsc，保存txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7db6ab115c439287a5d341353f7d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/modules/grouping.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7a876bf82d4d0d97720fd78fd8ad74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 文本文件路径\n",
    "tt_lst = [0, 1]\n",
    "for tt in tt_lst:\n",
    "    file_name = [\"sources.txt\", \"corrections.txt\"]\n",
    "    input_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_txt/jfleg/\" + file_name[tt]\n",
    "    output_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_txt/jfleg_corrected/\" + file_name[tt]\n",
    "\n",
    "    # 打开文件并读取内容到列表\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        jfleg_lst = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "    jfleg_lst_output = []\n",
    "    for item in tqdm(jfleg_lst):\n",
    "        corrected_string = cor.correct_text(item)\n",
    "        jfleg_lst_output.append(corrected_string)\n",
    "\n",
    "    # 将列表内容写入到txt文件中，最后一行不加换行符\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        for i, line in enumerate(jfleg_lst_output):\n",
    "            if i < len(jfleg_lst_output) - 1:\n",
    "                file.write(line + \"\\n\")\n",
    "            else:\n",
    "                file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2txt文件 rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6859366af0284fe7be40bd6219dcb187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/modules/grouping.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebef74a816f7481fa7b6ec964cddaea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n"
     ]
    }
   ],
   "source": [
    "# 文本文件路径\n",
    "score = 0.98    #确定阈值，小于阈值的WSC纠正进行回滚\n",
    "tt_lst = [0, 1]\n",
    "for tt in tt_lst:\n",
    "    file_name = [\"sources.txt\", \"corrections.txt\"]\n",
    "    input_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_txt/jfleg/\" + file_name[tt]\n",
    "    output_file_path_rollback = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_txt/jfleg_rollback/\" + file_name[tt]\n",
    "\n",
    "    # 打开文件并读取内容到列表\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        jfleg_lst = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    jfleg_lst_output_rollback = []\n",
    "    nn = 0\n",
    "    for item in tqdm(jfleg_lst):\n",
    "        ori_text = item\n",
    "        cor_text = cor.correct_text(item)\n",
    "        if calculate_semantic_similarity(ori_text, cor_text) < score:\n",
    "            jfleg_lst_output_rollback.append(ori_text)\n",
    "            nn += 1\n",
    "        else:\n",
    "            jfleg_lst_output_rollback.append(cor_text)\n",
    "\n",
    "    with open(output_file_path_rollback, \"w\") as file:\n",
    "        for i, line in enumerate(jfleg_lst_output_rollback):\n",
    "            if i < len(jfleg_lst_output_rollback) - 1:\n",
    "                file.write(line + \"\\n\")\n",
    "            else:\n",
    "                file.write(line)\n",
    "\n",
    "    print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1保存excel文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d291b2516c24cf08c6187cdd75c6726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujunhui/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/text_utils/modules/grouping.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64894ffb5bf1448c997de8bd6cae4034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e7cbeb279e4ea39e623f9338a1d5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe8101e9a6940c197f839a91974cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa857db729524821b3af24e3650733cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de28c1a7f5748d6812292e40928e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 文本文件路径\n",
    "score=0.98\n",
    "tt_lst = [0, 1]\n",
    "for tt in tt_lst:\n",
    "    file_name = [\"sources\", \"corrections\"]\n",
    "    input_file_path = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_txt/jfleg/\" + file_name[tt] + \".txt\"\n",
    "    output_file_path_1 = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_excel/jfleg/\" + file_name[tt] + \".xlsx\"\n",
    "    output_file_path_2 = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_excel/jfleg_corrected/\" + file_name[tt] + \".xlsx\"\n",
    "    output_file_path_3 = \"/home/liujunhui/workspace/proj/WSC/dataset/jfleg_excel/jfleg_rollback/\" + file_name[tt] + \".xlsx\"\n",
    "\n",
    "    # 打开文件并读取内容到列表\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        jfleg_lst = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "    # output_1\n",
    "    id_lst = []\n",
    "    jfleg_lst_output = []\n",
    "    counter = 1  # 初始化计数器\n",
    "    for item in tqdm(jfleg_lst):\n",
    "        id_lst.append(f\"a_{counter:04d}\")\n",
    "        corrected_string = cor.correct_text(item)\n",
    "        jfleg_lst_output.append(corrected_string)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    data = {\n",
    "        \"id\":id_lst,\n",
    "        \"text\":jfleg_lst_output\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_file_path_1, index=False)\n",
    "\n",
    "    # output_2\n",
    "    id_lst = []\n",
    "    jfleg_lst_output = []\n",
    "    counter = 1  # 初始化计数器\n",
    "    for item in tqdm(jfleg_lst):\n",
    "        id_lst.append(f\"a_{counter:04d}\")\n",
    "        corrected_string = cor.correct_text(item)\n",
    "        jfleg_lst_output.append(corrected_string)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    data = {\n",
    "        \"id\":id_lst,\n",
    "        \"text\":jfleg_lst_output\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_file_path_2, index=False)\n",
    "\n",
    "    # output_3\n",
    "    id_lst = []\n",
    "    jfleg_lst_output = []\n",
    "    counter = 1  # 初始化计数器\n",
    "    for item in tqdm(jfleg_lst):\n",
    "\n",
    "        ori_text = item\n",
    "        cor_text = cor.correct_text(item)\n",
    "        if calculate_semantic_similarity(ori_text, cor_text) < score:\n",
    "            id_lst.append(f\"a_{counter:04d}\")\n",
    "            jfleg_lst_output.append(ori_text)\n",
    "        else:\n",
    "            id_lst.append(f\"b_{counter:04d}\")\n",
    "            jfleg_lst_output.append(cor_text)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    data = {\n",
    "        \"id\":id_lst,\n",
    "        \"text\":jfleg_lst_output\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_file_path_3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
